{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpFD+lyEbwMAudkJ2BSSu0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrinal054/deep_learning_machine_learning/blob/master/pytorch/models/RAT3d/demo/rat_unet_keras_initialized_weights.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O65O-tY6Ms0C",
        "outputId": "f48aa2a5-0969-4575-a8be-d8cb9a60b480"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Concatenated shape at level two: torch.Size([3, 528, 16, 16, 16])\n",
            "Concatenated shape at level one: torch.Size([3, 288, 32, 32, 32])\n",
            "Concatenated shape at level zero: torch.Size([3, 192, 64, 64, 64])\n",
            "Concatenated shape at level two: torch.Size([2, 528, 16, 16, 16])\n",
            "Concatenated shape at level one: torch.Size([2, 288, 32, 32, 32])\n",
            "Concatenated shape at level zero: torch.Size([2, 192, 64, 64, 64])\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv3d-1       [-1, 32, 64, 64, 64]             896\n",
            "       BatchNorm3d-2       [-1, 32, 64, 64, 64]              64\n",
            "              ReLU-3       [-1, 32, 64, 64, 64]               0\n",
            "            Conv3d-4       [-1, 64, 64, 64, 64]          55,360\n",
            "       BatchNorm3d-5       [-1, 64, 64, 64, 64]             128\n",
            "            Conv3d-6       [-1, 64, 64, 64, 64]           4,160\n",
            "       BatchNorm3d-7       [-1, 64, 64, 64, 64]             128\n",
            "              ReLU-8       [-1, 64, 64, 64, 64]               0\n",
            "        conv_block-9       [-1, 64, 64, 64, 64]               0\n",
            "        MaxPool3d-10       [-1, 64, 32, 32, 32]               0\n",
            "           Conv3d-11       [-1, 64, 32, 32, 32]         110,656\n",
            "      BatchNorm3d-12       [-1, 64, 32, 32, 32]             128\n",
            "             ReLU-13       [-1, 64, 32, 32, 32]               0\n",
            "           Conv3d-14      [-1, 128, 32, 32, 32]         221,312\n",
            "      BatchNorm3d-15      [-1, 128, 32, 32, 32]             256\n",
            "           Conv3d-16      [-1, 128, 32, 32, 32]          16,512\n",
            "      BatchNorm3d-17      [-1, 128, 32, 32, 32]             256\n",
            "             ReLU-18      [-1, 128, 32, 32, 32]               0\n",
            "       conv_block-19      [-1, 128, 32, 32, 32]               0\n",
            "        Dropout3d-20      [-1, 128, 32, 32, 32]               0\n",
            "        MaxPool3d-21      [-1, 128, 16, 16, 16]               0\n",
            "           Conv3d-22      [-1, 128, 16, 16, 16]         442,496\n",
            "      BatchNorm3d-23      [-1, 128, 16, 16, 16]             256\n",
            "             ReLU-24      [-1, 128, 16, 16, 16]               0\n",
            "           Conv3d-25      [-1, 256, 16, 16, 16]         884,992\n",
            "      BatchNorm3d-26      [-1, 256, 16, 16, 16]             512\n",
            "           Conv3d-27      [-1, 256, 16, 16, 16]          65,792\n",
            "      BatchNorm3d-28      [-1, 256, 16, 16, 16]             512\n",
            "             ReLU-29      [-1, 256, 16, 16, 16]               0\n",
            "       conv_block-30      [-1, 256, 16, 16, 16]               0\n",
            "        Dropout3d-31      [-1, 256, 16, 16, 16]               0\n",
            "        MaxPool3d-32         [-1, 256, 8, 8, 8]               0\n",
            "           Conv3d-33         [-1, 256, 8, 8, 8]       1,769,728\n",
            "      BatchNorm3d-34         [-1, 256, 8, 8, 8]             512\n",
            "             ReLU-35         [-1, 256, 8, 8, 8]               0\n",
            "           Conv3d-36         [-1, 512, 8, 8, 8]       3,539,456\n",
            "      BatchNorm3d-37         [-1, 512, 8, 8, 8]           1,024\n",
            "           Conv3d-38         [-1, 512, 8, 8, 8]         262,656\n",
            "      BatchNorm3d-39         [-1, 512, 8, 8, 8]           1,024\n",
            "             ReLU-40         [-1, 512, 8, 8, 8]               0\n",
            "       conv_block-41         [-1, 512, 8, 8, 8]               0\n",
            "        Dropout3d-42         [-1, 512, 8, 8, 8]               0\n",
            "           Conv3d-43         [-1, 256, 8, 8, 8]         131,328\n",
            "      BatchNorm3d-44         [-1, 256, 8, 8, 8]             512\n",
            "             ReLU-45         [-1, 256, 8, 8, 8]               0\n",
            "           Conv3d-46         [-1, 256, 8, 8, 8]         524,544\n",
            "           Conv3d-47         [-1, 256, 8, 8, 8]          65,792\n",
            "  ConvTranspose3d-48         [-1, 256, 8, 8, 8]       1,769,728\n",
            "             ReLU-49         [-1, 256, 8, 8, 8]               0\n",
            "           Conv3d-50           [-1, 1, 8, 8, 8]             257\n",
            "          Sigmoid-51           [-1, 1, 8, 8, 8]               0\n",
            "         Upsample-52        [-1, 1, 16, 16, 16]               0\n",
            "           Conv3d-53       [-1, 16, 16, 16, 16]           4,112\n",
            "      BatchNorm3d-54       [-1, 16, 16, 16, 16]              32\n",
            "  attention_block-55       [-1, 16, 16, 16, 16]               0\n",
            "  ConvTranspose3d-56      [-1, 512, 16, 16, 16]       2,097,664\n",
            "           Conv3d-57      [-1, 256, 16, 16, 16]       3,649,792\n",
            "      BatchNorm3d-58      [-1, 256, 16, 16, 16]             512\n",
            "             ReLU-59      [-1, 256, 16, 16, 16]               0\n",
            "           Conv3d-60      [-1, 256, 16, 16, 16]       1,769,728\n",
            "      BatchNorm3d-61      [-1, 256, 16, 16, 16]             512\n",
            "           Conv3d-62      [-1, 256, 16, 16, 16]          65,792\n",
            "      BatchNorm3d-63      [-1, 256, 16, 16, 16]             512\n",
            "             ReLU-64      [-1, 256, 16, 16, 16]               0\n",
            "       conv_block-65      [-1, 256, 16, 16, 16]               0\n",
            "        Dropout3d-66      [-1, 256, 16, 16, 16]               0\n",
            "           Conv3d-67      [-1, 128, 16, 16, 16]          32,896\n",
            "      BatchNorm3d-68      [-1, 128, 16, 16, 16]             256\n",
            "             ReLU-69      [-1, 128, 16, 16, 16]               0\n",
            "           Conv3d-70      [-1, 128, 16, 16, 16]         131,200\n",
            "           Conv3d-71      [-1, 128, 16, 16, 16]          16,512\n",
            "  ConvTranspose3d-72      [-1, 128, 16, 16, 16]         442,496\n",
            "             ReLU-73      [-1, 128, 16, 16, 16]               0\n",
            "           Conv3d-74        [-1, 1, 16, 16, 16]             129\n",
            "          Sigmoid-75        [-1, 1, 16, 16, 16]               0\n",
            "         Upsample-76        [-1, 1, 32, 32, 32]               0\n",
            "           Conv3d-77       [-1, 32, 32, 32, 32]           4,128\n",
            "      BatchNorm3d-78       [-1, 32, 32, 32, 32]              64\n",
            "  attention_block-79       [-1, 32, 32, 32, 32]               0\n",
            "  ConvTranspose3d-80      [-1, 256, 32, 32, 32]         524,544\n",
            "           Conv3d-81      [-1, 128, 32, 32, 32]         995,456\n",
            "      BatchNorm3d-82      [-1, 128, 32, 32, 32]             256\n",
            "             ReLU-83      [-1, 128, 32, 32, 32]               0\n",
            "           Conv3d-84      [-1, 128, 32, 32, 32]         442,496\n",
            "      BatchNorm3d-85      [-1, 128, 32, 32, 32]             256\n",
            "           Conv3d-86      [-1, 128, 32, 32, 32]          16,512\n",
            "      BatchNorm3d-87      [-1, 128, 32, 32, 32]             256\n",
            "             ReLU-88      [-1, 128, 32, 32, 32]               0\n",
            "       conv_block-89      [-1, 128, 32, 32, 32]               0\n",
            "        Dropout3d-90      [-1, 128, 32, 32, 32]               0\n",
            "           Conv3d-91       [-1, 64, 32, 32, 32]           8,256\n",
            "      BatchNorm3d-92       [-1, 64, 32, 32, 32]             128\n",
            "             ReLU-93       [-1, 64, 32, 32, 32]               0\n",
            "           Conv3d-94       [-1, 64, 32, 32, 32]          32,832\n",
            "           Conv3d-95       [-1, 64, 32, 32, 32]           4,160\n",
            "  ConvTranspose3d-96       [-1, 64, 32, 32, 32]         110,656\n",
            "             ReLU-97       [-1, 64, 32, 32, 32]               0\n",
            "           Conv3d-98        [-1, 1, 32, 32, 32]              65\n",
            "          Sigmoid-99        [-1, 1, 32, 32, 32]               0\n",
            "        Upsample-100        [-1, 1, 64, 64, 64]               0\n",
            "          Conv3d-101       [-1, 64, 64, 64, 64]           4,160\n",
            "     BatchNorm3d-102       [-1, 64, 64, 64, 64]             128\n",
            " attention_block-103       [-1, 64, 64, 64, 64]               0\n",
            " ConvTranspose3d-104      [-1, 128, 64, 64, 64]         131,200\n",
            "          Conv3d-105       [-1, 64, 64, 64, 64]         331,840\n",
            "     BatchNorm3d-106       [-1, 64, 64, 64, 64]             128\n",
            "            ReLU-107       [-1, 64, 64, 64, 64]               0\n",
            "          Conv3d-108       [-1, 64, 64, 64, 64]         110,656\n",
            "     BatchNorm3d-109       [-1, 64, 64, 64, 64]             128\n",
            "          Conv3d-110       [-1, 64, 64, 64, 64]           4,160\n",
            "     BatchNorm3d-111       [-1, 64, 64, 64, 64]             128\n",
            "            ReLU-112       [-1, 64, 64, 64, 64]               0\n",
            "      conv_block-113       [-1, 64, 64, 64, 64]               0\n",
            "       Dropout3d-114       [-1, 64, 64, 64, 64]               0\n",
            "          Conv3d-115        [-1, 2, 64, 64, 64]             130\n",
            "================================================================\n",
            "Total params: 20,805,845\n",
            "Trainable params: 20,805,845\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.00\n",
            "Forward/backward pass size (MB): 3917.35\n",
            "Params size (MB): 79.37\n",
            "Estimated Total Size (MB): 3997.72\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Aug 21 21:51:29 2022\n",
        "\n",
        "@author: mrinal\n",
        "\n",
        "rat_unet aka ra2_unet\n",
        "\n",
        "v2:\n",
        "    Weights are assigned for individual conv3d layers and conv3dtranspose layers\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def activations(activation: str):\n",
        "    ''' Choose the activation function '''\n",
        "    \n",
        "    if activation == 'relu': return nn.ReLU(inplace=True)\n",
        "    elif activation == 'leaky': return nn.LeakyReLU(negative_slope=0.1)\n",
        "    elif activation == 'elu': return nn.ELU()\n",
        "    elif activation == 'sigmoid': return nn.Sigmoid()\n",
        "    elif activation == 'softmax': return nn.Softmax(dim=1)\n",
        "    else: raise ValueError('Wrong keyword for activation')\n",
        "\n",
        "def normalization(norm: str, n_channel):\n",
        "    ''' Choose type of normalization '''\n",
        "    \n",
        "    if norm == 'batch': return nn.BatchNorm3d(n_channel)\n",
        "    elif norm == 'instance': return nn.InstanceNorm3d(n_channel)\n",
        "    elif norm == None: pass # do nothing\n",
        "    else: raise ValueError('Wrong keyword for normalization') \n",
        "\n",
        "# Weight initialization with keras default value\n",
        "def weight_init_keras_default(m):\n",
        "    if isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose3d):\n",
        "        nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
        "        nn.init.zeros_(m.bias)\n",
        "\n",
        "# Weight initialization with truncated normal\n",
        "def weight_init(m):\n",
        "    if isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose3d):\n",
        "        torch.nn.init.trunc_normal_(m.weight, std=0.1)\n",
        "        m.bias.data.zero_()\n",
        "            \n",
        "class conv_block(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channel, stage1_out_channel, multiplier: int, norm: str, activation, pad='same'):\n",
        "        super(conv_block, self).__init__()\n",
        "        \n",
        "        ''' It performs conv-norm-activation in two stages. For the second stage,\n",
        "            stage 2 output_channel = stage 1 output_channel x multiplier. Finally,\n",
        "            it applies residual connection. '''\n",
        "        \n",
        "        # Get output channels for the 2nd stage\n",
        "        stage2_out_channel = multiplier * stage1_out_channel\n",
        "        \n",
        "        # Apply conv --> normalization--> activation (cna)\n",
        "        self.double_cna = nn.Sequential(\n",
        "            # Stage 1\n",
        "            nn.Conv3d(in_channel, stage1_out_channel, kernel_size=3, padding=pad),\n",
        "            normalization(norm, stage1_out_channel),\n",
        "            activations(activation),\n",
        "            \n",
        "            # Stage 2\n",
        "            nn.Conv3d(stage1_out_channel, stage2_out_channel, kernel_size=3, padding=pad),\n",
        "            normalization(norm, stage2_out_channel),    \n",
        "            # No activation right now                \n",
        "            )\n",
        "        \n",
        "        self.double_cna.apply(weight_init) # initializing with truncated normal\n",
        "        \n",
        "        # Resnet Connection\n",
        "        self.shortcut = nn.Sequential(\n",
        "            nn.Conv3d(stage2_out_channel, stage2_out_channel, kernel_size=1, stride=1, padding='same', bias=True),\n",
        "            normalization(norm, stage2_out_channel),\n",
        "            )\n",
        "        \n",
        "        self.shortcut.apply(weight_init) # initializing with truncated normal\n",
        "        \n",
        "        # Here add two tensors\n",
        "        \n",
        "        self.res_activation =  activations(activation)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x1 = self.double_cna(x)\n",
        "        x2 = self.shortcut(x1)\n",
        "        x3 = x2 + x1\n",
        "        x3 = self.res_activation(x3)\n",
        "        \n",
        "        return x3\n",
        "\n",
        "def gating_signal(in_channel, out_channel, activation: str, norm=None):\n",
        "\n",
        "    gate = nn.Sequential(\n",
        "        nn.Conv3d(in_channel, out_channel, kernel_size=1, stride=1, padding='same'),\n",
        "        normalization(norm, out_channel),\n",
        "        activations(activation),\n",
        "        )\n",
        "    gate.apply(weight_init_keras_default)\n",
        "    \n",
        "    return gate\n",
        "\n",
        "class attention_block(nn.Module):\n",
        "    def __init__(self, F_int, shape):\n",
        "        super(attention_block, self).__init__()\n",
        "        \n",
        "        self.F_int = F_int\n",
        "        \n",
        "        self.theta_x = nn.Conv3d(F_int, F_int, kernel_size=2, stride=2, padding=0, bias=True)\n",
        "        self.theta_x.apply(weight_init_keras_default)\n",
        "        \n",
        "        self.phi_g = nn.Conv3d(F_int, F_int, kernel_size=1, padding='same')\n",
        "        self.phi_g.apply(weight_init_keras_default)\n",
        "        \n",
        "        self.upsample_g = nn.ConvTranspose3d(F_int, F_int, kernel_size=3, stride=1, padding=1)\n",
        "        self.upsample_g.apply(weight_init_keras_default)\n",
        "        \n",
        "        # Here is the concatenation step\n",
        "        \n",
        "        self.act_xg = nn.ReLU(inplace=True)\n",
        "        \n",
        "        self.psi = nn.Conv3d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True)\n",
        "        self.psi.apply(weight_init_keras_default)\n",
        "        \n",
        "        self.sigmoid_xg = nn.Sigmoid()\n",
        "        \n",
        "        self.upsample_psi = nn.Upsample(scale_factor=2)\n",
        "        \n",
        "        # Here is repeat elements. upsample_psi has 1 ch only. Repeat it F_int times.\n",
        "        \n",
        "        # Here is multiplication between x and upsample_psi\n",
        "        \n",
        "        self.result = nn.Conv3d(F_int, shape, kernel_size=1, padding='same')\n",
        "        self.result.apply(weight_init_keras_default)\n",
        "        \n",
        "        self.result_bn = nn.BatchNorm3d(shape)\n",
        "         \n",
        "    def forward(self, g, x):\n",
        "        theta_x = self.theta_x(x)\n",
        "        phi_g = self.phi_g(g)\n",
        "        upsample_g = self.upsample_g(phi_g)        \n",
        "        concat_xg = upsample_g + theta_x\n",
        "        act_xg = self.act_xg(concat_xg)\n",
        "        psi = self.psi(act_xg)\n",
        "        sigmoid_xg = self.sigmoid_xg(psi)\n",
        "        upsample_psi = self.upsample_psi(sigmoid_xg)\n",
        "        upsample_psi = torch.repeat_interleave(upsample_psi, self.F_int, dim=1)\n",
        "        y = upsample_psi * x\n",
        "        result = self.result(y)\n",
        "        result_bn = self.result_bn(result)\n",
        "        \n",
        "        return result_bn\n",
        "           \n",
        "    \n",
        "def up_conv(in_channel, out_channel, kernel, stride, pad=0):\n",
        "    ''' It performs up sampling that is needed in the decoder '''\n",
        "    \n",
        "    up_convolution = nn.ConvTranspose3d(\n",
        "        in_channel, out_channel, kernel_size=kernel, stride=stride, padding=pad)    \n",
        "    \n",
        "    up_convolution.apply(weight_init) # initializing with truncated normal\n",
        "    \n",
        "    return up_convolution\n",
        "    \n",
        "def center_crop_and_concat(encoder_tensor, decoder_tensor):\n",
        "    '''\n",
        "    It first crops encoder tensor to match with decoder tensor size. Required when padding=0. \n",
        "    Then it concatenates the cropped encoder tensor and the decoder tensor. \n",
        "    \n",
        "    Input\n",
        "    ------\n",
        "    encoder_tensor: Encoder tensor (source tensor)\n",
        "    decoder_tensor: Decoder tensor (target tensor)\n",
        "    \n",
        "    Output\n",
        "    ------\n",
        "    Concatenation of cropped encoder tensor and decoder tensor\n",
        "    '''\n",
        "    \n",
        "    encoder_size = encoder_tensor.size()[2:] # depth, height, and width only\n",
        "    decoder_size = decoder_tensor.size()[2:] # depth, height, and width only\n",
        "    \n",
        "    cropped_encoder_tensor = encoder_tensor[\n",
        "                :,  # batch\n",
        "                :,  # channel \n",
        "                ((encoder_size[0] - decoder_size[0]) // 2):((encoder_size[0] + decoder_size[0]) // 2),\n",
        "                ((encoder_size[1] - decoder_size[1]) // 2):((encoder_size[1] + decoder_size[1]) // 2),\n",
        "                ((encoder_size[2] - decoder_size[2]) // 2):((encoder_size[2] + decoder_size[2]) // 2)\n",
        "                ]\n",
        "    \n",
        "    return torch.cat([cropped_encoder_tensor, decoder_tensor], 1)     \n",
        "\n",
        "class RATUNet3D(nn.Module):\n",
        "    def __init__(\n",
        "            self, input_shape, base_feature, out_channel:int, multiplier: int, norm: str,  \n",
        "            in_activation: str, out_activation: str, dropout:float, pad='same'):\n",
        "        '''\n",
        "        Input\n",
        "        ------\n",
        "        input_channel: (int) No. of input channels of the original data. (e.g. 1)\n",
        "        base_feature: (int) No. of out channels of the first convolution. (e.g. 32)\n",
        "        out_channel: (int) No. of channels of final output (e.g. 2)\n",
        "        multiplier: (int) Whether to double no. of out channels. It can be either 1 or 2.\n",
        "        norm: (str) Normalization. Keyword: 'batch', or 'instance'\n",
        "        in_activation: (str) Activation function. Keyword: 'relu', 'leaky', or 'elu'\n",
        "        out_activation: (str) Activation function applied to output layer. (e.g. 'softmax' or None)\n",
        "                If out_activation=None, then no activation will be applied to the output layer\n",
        "        dropout: (float) Dropout\n",
        "        pad: (str) Padding. Either 'same' or 'valid'\n",
        "             \n",
        "        Output\n",
        "        -------\n",
        "        Output tensor\n",
        "        '''\n",
        "        \n",
        "        super(RATUNet3D, self).__init__()\n",
        "        \n",
        "        if multiplier not in [1, 2]: raise ValueError('value of multiplier can only be 1 or 2')\n",
        "        \n",
        "        self.dropout = nn.Dropout3d(p=dropout)\n",
        "        self.max_pool = nn.MaxPool3d(kernel_size=2, stride=2) \n",
        "        \n",
        "        # Level zero (note: suffix 'e' means encoder and 'd' means decoder)\n",
        "        self.conv_e0 = conv_block(input_shape[0], base_feature, multiplier, norm, in_activation, pad)\n",
        "        out_channel_encoder_0 = base_feature * multiplier #64 if multiplier=2\n",
        "\n",
        "        # Level one \n",
        "        in_channel_encoder_1 = base_feature * 2 #64\n",
        "        self.conv_e1 = conv_block(in_channel_encoder_1, in_channel_encoder_1, multiplier, norm, in_activation, pad)\n",
        "        out_channel_encoder_1 = in_channel_encoder_1 * multiplier #128 if multiplier=2\n",
        "        \n",
        "        # Level two\n",
        "        in_channel_encoder_2 = in_channel_encoder_1 * 2 #128\n",
        "        self.conv_e2 = conv_block(in_channel_encoder_2, in_channel_encoder_2, multiplier, norm, in_activation, pad)\n",
        "        out_channel_encoder_2 = in_channel_encoder_2 * multiplier #256 if multiplier=2\n",
        "        \n",
        "        # Level three\n",
        "        in_channel_encoder_3 = in_channel_encoder_2 * 2 #256\n",
        "        self.conv_e3 = conv_block(in_channel_encoder_3, in_channel_encoder_3, multiplier, norm, in_activation, pad)\n",
        "        out_channel_encoder_3 = in_channel_encoder_3 * multiplier #512 if multiplier=2\n",
        "           \n",
        "        # Level two\n",
        "        L = 2        \n",
        "        shape_2 = int(input_shape[1]/(2**L)) #16\n",
        "        in_channel_decoder_2 = out_channel_encoder_3 + shape_2 #512+16 = 528\n",
        "        self.gating_1 = gating_signal(out_channel_encoder_3, out_channel_encoder_2, activation='relu', norm='batch')  # ******\n",
        "        self.attn_1c = attention_block(out_channel_encoder_2, shape=shape_2)\n",
        "        self.upconv_2 = up_conv(out_channel_encoder_3, out_channel_encoder_3, kernel=2, stride=2, pad=0)      \n",
        "        self.conv_d2 = conv_block(in_channel_decoder_2, out_channel_encoder_2, 1, norm, in_activation, pad) # multiplier=1                         \n",
        "        out_channel_decoder_2 = out_channel_encoder_2 #256\n",
        "        \n",
        "        # Level one\n",
        "        L = 1\n",
        "        shape_1 = int(input_shape[1]/(2**L)) #32\n",
        "        in_channel_decoder_1 = out_channel_decoder_2 + shape_1 #256+32 = 288\n",
        "        self.gating_2 = gating_signal(out_channel_decoder_2, out_channel_encoder_1, activation='relu', norm='batch')  # ******\n",
        "        self.attn_2c = attention_block(out_channel_encoder_1, shape=shape_1)\n",
        "        self.upconv_1 = up_conv(out_channel_decoder_2, out_channel_decoder_2, kernel=2, stride=2, pad=0)  \n",
        "        self.conv_d1 = conv_block(in_channel_decoder_1, out_channel_encoder_1, 1, norm, in_activation, pad) # multiplier=1                                  \n",
        "        out_channel_decoder_1 = out_channel_encoder_1 #128\n",
        "        \n",
        "        # Level zero\n",
        "        L = 0\n",
        "        shape_0 = int(input_shape[1]/(2**L)) #64\n",
        "        in_channel_decoder_0 = out_channel_decoder_1 + shape_0 #128+64 = 192\n",
        "        self.gating_3 = gating_signal(out_channel_decoder_1, out_channel_encoder_0, activation='relu', norm='batch')  # ******\n",
        "        self.attn_3c = attention_block(out_channel_encoder_0, shape=shape_0)\n",
        "        self.upconv_0 = up_conv(out_channel_decoder_1, out_channel_decoder_1, kernel=2, stride=2, pad=0) \n",
        "        self.conv_d0 = conv_block(in_channel_decoder_0, out_channel_encoder_0, 1, norm, in_activation, pad) # multiplier=1                                  \n",
        "        out_channel_decoder_0 = out_channel_encoder_0\n",
        "        \n",
        "        # Output\n",
        "        if out_activation is None:\n",
        "            self.out = nn.Conv3d(out_channel_decoder_0, out_channel, kernel_size=1, stride=1, padding=pad)\n",
        "        else:\n",
        "            self.out = nn.Sequential(\n",
        "                nn.Conv3d(out_channel_decoder_0, out_channel, kernel_size=1, stride=1, padding=pad),\n",
        "                activations(out_activation)\n",
        "                )\n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        # x = x.cpu()\n",
        "        \n",
        "        # Level zero\n",
        "        conv_e0 = self.conv_e0(x)\n",
        "        \n",
        "        # Level one\n",
        "        maxpool_1 = self.max_pool(conv_e0)\n",
        "        conv_e1 = self.conv_e1(maxpool_1)\n",
        "        conv_e1 = self.dropout(conv_e1)\n",
        "        \n",
        "        # Level two\n",
        "        maxpool_2 = self.max_pool(conv_e1)\n",
        "        conv_e2 = self.conv_e2(maxpool_2)\n",
        "        conv_e2 = self.dropout(conv_e2)\n",
        "       \n",
        "        # Level three\n",
        "        maxpool_3 = self.max_pool(conv_e2)\n",
        "        conv_e3 = self.conv_e3(maxpool_3)\n",
        "        conv_e3 = self.dropout(conv_e3)\n",
        "        \n",
        "        # Level two\n",
        "        gating_1 = self.gating_1(conv_e3)        \n",
        "        attn_1 = self.attn_1c(g=gating_1, x=conv_e2)\n",
        "        upconv_2 = self.upconv_2(conv_e3)\n",
        "        concat_2 = center_crop_and_concat(attn_1, upconv_2)\n",
        "        print('Concatenated shape at level two:', concat_2.shape)\n",
        "        conv_d2 = self.conv_d2(concat_2)\n",
        "        conv_d2 = self.dropout(conv_d2)\n",
        "        \n",
        "        # Level one\n",
        "        gating_2 = self.gating_2(conv_d2)       \n",
        "        attn_2 = self.attn_2c(g=gating_2, x=conv_e1)\n",
        "        upconv_1 = self.upconv_1(conv_d2)\n",
        "        concat_1 = center_crop_and_concat(attn_2, upconv_1)\n",
        "        print('Concatenated shape at level one:', concat_1.shape)\n",
        "        conv_d1 = self.conv_d1(concat_1)\n",
        "        conv_d1 = self.dropout(conv_d1)\n",
        "        \n",
        "        # Level zero\n",
        "        gating_3 = self.gating_3(conv_d1)  \n",
        "        attn_3 = self.attn_3c(g=gating_3, x=conv_e0)\n",
        "        upconv_0 = self.upconv_0(conv_d1)       \n",
        "        concat_0 = center_crop_and_concat(attn_3, upconv_0)\n",
        "        print('Concatenated shape at level zero:', concat_0.shape)\n",
        "        conv_d0 = self.conv_d0(concat_0)\n",
        "        conv_d0 = self.dropout(conv_d0)\n",
        "        \n",
        "        # Output\n",
        "        out = self.out(conv_d0)\n",
        "        \n",
        "        return(out)\n",
        "\n",
        "    \n",
        "# def weight_init(m):\n",
        "#     if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "#         nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
        "#         nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "# Test model            \n",
        "if __name__ == \"__main__\":\n",
        "    from torchsummary import summary \n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    base_feature = 32\n",
        "    img = torch.rand(3, 1, 64, 64, 64).to(device) # batch_size, channel, height, width\n",
        "    input_feature = img.size()[1]\n",
        "    \n",
        "    input_shape = (1, 64, 64, 64)\n",
        "    \n",
        "    model = RATUNet3D(input_shape, base_feature, out_channel=2, multiplier=2, \n",
        "    norm='batch', in_activation='relu', out_activation=None, dropout=0.15, pad='same')\n",
        "    \n",
        "    model = model.to(device)\n",
        "    \n",
        "    # model.apply(weight_init)\n",
        "    \n",
        "    out = model(img)\n",
        "    \n",
        "    summary(model, input_shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention: torchsummary"
      ],
      "metadata": {
        "id": "SNRHXY6mSadM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The version of `torchsummary` used in this demo is `1.5.1`. The installed version of `torchsummary` won't work here. It will throw some tuple index error. To fix the problem, replace the code of `torchsummary.py` with the following code. Don't forget to uncomment the code before replacing."
      ],
      "metadata": {
        "id": "dbqRGS4zShbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torch.autograd import Variable\n",
        "\n",
        "# from collections import OrderedDict\n",
        "# import numpy as np\n",
        "\n",
        "\n",
        "# def summary(model, input_size, batch_size=-1, device=\"cuda\"):\n",
        "\n",
        "#     def register_hook(module):\n",
        "\n",
        "#         def hook(module, input, output):\n",
        "#             class_name = str(module.__class__).split(\".\")[-1].split(\"'\")[0]\n",
        "#             module_idx = len(summary)\n",
        "\n",
        "#             m_key = \"%s-%i\" % (class_name, module_idx + 1)\n",
        "#             summary[m_key] = OrderedDict()\n",
        "           \n",
        "#             # following part is moderated by https://github.com/graykode/modelsummary/issues/1\n",
        "#             if len(input) != 0:\n",
        "#                 summary[m_key][\"input_shape\"] = list(input[0].size())\n",
        "#                 summary[m_key][\"input_shape\"][0] = batch_size\n",
        "#             else: summary[m_key][\"input_shape\"] = input\n",
        "            \n",
        "#             if isinstance(output, (list, tuple)):\n",
        "#                 summary[m_key][\"output_shape\"] = [\n",
        "#                     [-1] + list(o.size())[1:] for o in output\n",
        "#                 ]\n",
        "#             else:\n",
        "#                 summary[m_key][\"output_shape\"] = list(output.size())\n",
        "#                 summary[m_key][\"output_shape\"][0] = batch_size\n",
        "\n",
        "#             params = 0\n",
        "#             if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n",
        "#                 params += torch.prod(torch.LongTensor(list(module.weight.size())))\n",
        "#                 summary[m_key][\"trainable\"] = module.weight.requires_grad\n",
        "#             if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n",
        "#                 params += torch.prod(torch.LongTensor(list(module.bias.size())))\n",
        "#             summary[m_key][\"nb_params\"] = params\n",
        "\n",
        "#         if (\n",
        "#             not isinstance(module, nn.Sequential)\n",
        "#             and not isinstance(module, nn.ModuleList)\n",
        "#             and not (module == model)\n",
        "#         ):\n",
        "#             hooks.append(module.register_forward_hook(hook))\n",
        "\n",
        "#     device = device.lower()\n",
        "#     assert device in [\n",
        "#         \"cuda\",\n",
        "#         \"cpu\",\n",
        "#     ], \"Input device is not valid, please specify 'cuda' or 'cpu'\"\n",
        "\n",
        "#     if device == \"cuda\" and torch.cuda.is_available():\n",
        "#         dtype = torch.cuda.FloatTensor\n",
        "#     else:\n",
        "#         dtype = torch.FloatTensor\n",
        "\n",
        "#     # multiple inputs to the network\n",
        "#     if isinstance(input_size, tuple):\n",
        "#         input_size = [input_size]\n",
        "\n",
        "#     # batch_size of 2 for batchnorm\n",
        "#     x = [torch.rand(2, *in_size).type(dtype) for in_size in input_size]\n",
        "#     # print(type(x[0]))\n",
        "\n",
        "#     # create properties\n",
        "#     summary = OrderedDict()\n",
        "#     hooks = []\n",
        "\n",
        "#     # register hook\n",
        "#     model.apply(register_hook)\n",
        "\n",
        "#     # make a forward pass\n",
        "#     # print(x.shape)\n",
        "#     model(*x)\n",
        "\n",
        "#     # remove these hooks\n",
        "#     for h in hooks:\n",
        "#         h.remove()\n",
        "\n",
        "#     print(\"----------------------------------------------------------------\")\n",
        "#     line_new = \"{:>20}  {:>25} {:>15}\".format(\"Layer (type)\", \"Output Shape\", \"Param #\")\n",
        "#     print(line_new)\n",
        "#     print(\"================================================================\")\n",
        "#     total_params = 0\n",
        "#     total_output = 0\n",
        "#     trainable_params = 0\n",
        "#     for layer in summary:\n",
        "#         # input_shape, output_shape, trainable, nb_params\n",
        "#         line_new = \"{:>20}  {:>25} {:>15}\".format(\n",
        "#             layer,\n",
        "#             str(summary[layer][\"output_shape\"]),\n",
        "#             \"{0:,}\".format(summary[layer][\"nb_params\"]),\n",
        "#         )\n",
        "#         total_params += summary[layer][\"nb_params\"]\n",
        "#         total_output += np.prod(summary[layer][\"output_shape\"])\n",
        "#         if \"trainable\" in summary[layer]:\n",
        "#             if summary[layer][\"trainable\"] == True:\n",
        "#                 trainable_params += summary[layer][\"nb_params\"]\n",
        "#         print(line_new)\n",
        "\n",
        "#     # assume 4 bytes/number (float on cuda).\n",
        "#     total_input_size = abs(np.prod(input_size) * batch_size * 4. / (1024 ** 2.))\n",
        "#     total_output_size = abs(2. * total_output * 4. / (1024 ** 2.))  # x2 for gradients\n",
        "#     total_params_size = abs(total_params.numpy() * 4. / (1024 ** 2.))\n",
        "#     total_size = total_params_size + total_output_size + total_input_size\n",
        "\n",
        "#     print(\"================================================================\")\n",
        "#     print(\"Total params: {0:,}\".format(total_params))\n",
        "#     print(\"Trainable params: {0:,}\".format(trainable_params))\n",
        "#     print(\"Non-trainable params: {0:,}\".format(total_params - trainable_params))\n",
        "#     print(\"----------------------------------------------------------------\")\n",
        "#     print(\"Input size (MB): %0.2f\" % total_input_size)\n",
        "#     print(\"Forward/backward pass size (MB): %0.2f\" % total_output_size)\n",
        "#     print(\"Params size (MB): %0.2f\" % total_params_size)\n",
        "#     print(\"Estimated Total Size (MB): %0.2f\" % total_size)\n",
        "#     print(\"----------------------------------------------------------------\")\n",
        "#     # return summary\n"
      ],
      "metadata": {
        "id": "ESY1eShvSLeE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}