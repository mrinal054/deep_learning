# -*- coding: utf-8 -*-
"""
Created on Sun Aug 21 21:51:29 2022

Residual-attention UNet (RAUNet)

@author: mrinal
"""
import torch
import torch.nn as nn

def activations(activation: str):
    ''' Choose the activation function '''
    
    if activation == 'relu': return nn.ReLU(inplace=True)
    elif activation == 'leaky': return nn.LeakyReLU(negative_slope=0.1)
    elif activation == 'elu': return nn.ELU()
    elif activation == 'sigmoid': return nn.Sigmoid()
    elif activation == 'softmax': return nn.Softmax(dim=1)
    else: raise ValueError('Wrong keyword for activation')

def normalization(norm: str, n_channel):
    ''' Choose type of normalization '''
    
    if norm == 'batch': return nn.BatchNorm3d(n_channel)
    elif norm == 'instance': return nn.InstanceNorm3d(n_channel)
    elif norm == None: pass # do nothing
    else: raise ValueError('Wrong keyword for normalization') 

class conv_block(nn.Module):
    def __init__(self, in_channel, stage1_out_channel, multiplier: int, norm: str, activation, pad='same'):
        super(conv_block, self).__init__()
        
        ''' It performs conv-norm-activation in two stages. For the second stage,
            stage 2 output_channel = stage 1 output_channel x multiplier. Finally,
            it applies residual connection. '''
        
        # Get output channels for the 2nd stage
        stage2_out_channel = multiplier * stage1_out_channel
        
        # Apply conv --> normalization--> activation (cna)
        self.double_cna = nn.Sequential(
            # Stage 1
            nn.Conv3d(in_channel, stage1_out_channel, kernel_size=3, padding=pad),
            normalization(norm, stage1_out_channel),
            activations(activation),
            
            # Stage 2
            nn.Conv3d(stage1_out_channel, stage2_out_channel, kernel_size=3, padding=pad),
            normalization(norm, stage2_out_channel),    
            # No activation right now                
            )
        
        # Resnet Connection
        self.shortcut = nn.Sequential(
            nn.Conv3d(in_channel, stage2_out_channel, kernel_size=1, stride=1, padding='same', bias=True),
            normalization(norm, stage2_out_channel),
            )
        
        # Here add two tensors
        
        self.res_activation =  activations(activation)
        
    def forward(self, x):
        x1 = self.double_cna(x)
        x2 = self.shortcut(x)
        x3 = x2 + x1
        x3 = self.res_activation(x3)
        
        return x3

def gating_signal(in_channel, out_channel, activation: str, norm=None):

    return nn.Sequential(
        nn.Conv3d(in_channel, out_channel, kernel_size=1, stride=1, padding='same'),
        normalization(norm, out_channel),
        activations(activation),
        )


class attention_block(nn.Module):
    def __init__(self, F_int, shape):
        super(attention_block, self).__init__()
        
        self.F_int = F_int
        
        self.theta_x = nn.Conv3d(F_int, F_int, kernel_size=2, stride=2, padding=0, bias=True)
        
        self.phi_g = nn.Conv3d(F_int, F_int, kernel_size=1, padding='same')
        
        self.upsample_g = nn.ConvTranspose3d(F_int, F_int, kernel_size=3, stride=1, padding=1)
        
        # Here is the concatenation step
        
        self.act_xg = nn.ReLU(inplace=True)
        
        self.psi = nn.Conv3d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True)
        
        self.sigmoid_xg = nn.Sigmoid()
        
        self.upsample_psi = nn.Upsample(scale_factor=2)
        
        # Here is repeat elements. upsample_psi has 1 ch only. Repeat it F_int times.
        
        # Here is multiplication between x and upsample_psi
        
        self.result = nn.Conv3d(F_int, shape, kernel_size=1, padding='same')
        
        self.result_bn = nn.BatchNorm3d(shape)
        
        
    def forward(self, g, x):
        theta_x = self.theta_x(x)
        phi_g = self.phi_g(g)
        upsample_g = self.upsample_g(phi_g)        
        concat_xg = upsample_g + theta_x
        act_xg = self.act_xg(concat_xg)
        psi = self.psi(act_xg)
        sigmoid_xg = self.sigmoid_xg(psi)
        upsample_psi = self.upsample_psi(sigmoid_xg)
        upsample_psi = torch.repeat_interleave(upsample_psi, self.F_int, dim=1)
        y = upsample_psi * x
        result = self.result(y)
        result_bn = self.result_bn(result)
        
        return result_bn
           
