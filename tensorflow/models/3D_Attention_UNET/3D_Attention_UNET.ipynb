{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1VEZ4gk3rU5X"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import scipy.io as sio\n",
    "%matplotlib notebook\n",
    "# Required modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 64 # Input feature width/height\n",
    "INPUT_DEPTH = 64 # Input depth \n",
    "INPUT_CHANNEL = 1\n",
    "OUTPUT_SIZE = 64 # Output feature width/height \n",
    "OUTPUT_DEPTH = 64 # Output depth\n",
    "OUTPUT_CHANNEL = 1\n",
    "OUTPUT_CLASSES = 4 # Number of output classes in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_wHLvMbrU5j"
   },
   "outputs": [],
   "source": [
    "OFF_IMAGE_FILL = 0 # What to fill an image with if padding is required to make Tensor\n",
    "OFF_LABEL_FILL = 0 # What to fill a label with if padding is required to make Tensor\n",
    "\n",
    "# Get 'natural' OUTPUT_DEPTH according to scipy method\n",
    "io_zoom = OUTPUT_SIZE/INPUT_SIZE\n",
    "zero_chk = np.zeros((INPUT_SIZE, INPUT_SIZE, INPUT_DEPTH))\n",
    "\n",
    "def get_scaled_input(data, min_i = INPUT_SIZE, min_o = OUTPUT_SIZE, depth = INPUT_DEPTH, \n",
    "                    depth_out = OUTPUT_DEPTH, image_fill = OFF_IMAGE_FILL, \n",
    "                    label_fill = OFF_LABEL_FILL, n_classes = OUTPUT_CLASSES, norm_max = 500):\n",
    "    \n",
    "    # Takes raw data (x, y) and scales to match desired input and output sizes to feed into Tensorflow\n",
    "    # Pads and normalises input and also moves axes around to orientation expected by tensorflow\n",
    "    \n",
    "    input_scale_factor = min_i/data[0].shape[0]\n",
    "    output_scale_factor = min_o/data[0].shape[0]\n",
    "\n",
    "    vox_zoom = None\n",
    "    lbl_zoom = None\n",
    "\n",
    "    if not input_scale_factor == 1:\n",
    "        vox_zoom = scipy.ndimage.interpolation.zoom(data[0], input_scale_factor, order = 1) \n",
    "        # Order 1 is bilinear - fast and good enough\n",
    "    else:\n",
    "        vox_zoom = data[0]\n",
    "\n",
    "    if not output_scale_factor == 1:\n",
    "        lbl_zoom = scipy.ndimage.interpolation.zoom(data[1], output_scale_factor, order = 0) \n",
    "        # Order 0 is nearest neighbours: VERY IMPORTANT as it ensures labels are scaled properly (and stay discrete)\n",
    "    else:\n",
    "        lbl_zoom = data[1]   \n",
    "\n",
    "    lbl_pad = label_fill*np.ones((min_o, min_o, depth_out - lbl_zoom.shape[-1]))\n",
    "    lbl_zoom = np.concatenate((lbl_zoom, lbl_pad), 2)\n",
    "    lbl_zoom = lbl_zoom[np.newaxis, :, :, :]\n",
    "    \n",
    "    vox_pad = image_fill*np.ones((min_i, min_i, depth - vox_zoom.shape[-1]))\n",
    "    vox_zoom = np.concatenate((vox_zoom, vox_pad), 2)\n",
    "    \n",
    "    max_val = np.max(vox_zoom)\n",
    "    if not np.max(vox_zoom) == 0:\n",
    "        vox_zoom = vox_zoom * norm_max/np.max(vox_zoom)\n",
    "        \n",
    "    vox_zoom = vox_zoom[np.newaxis, :, :, :]\n",
    "\n",
    "    vox_zoom = np.swapaxes(vox_zoom, 0, -1)\n",
    "    lbl_zoom = np.swapaxes(lbl_zoom, 0, -1)\n",
    "    # Swap axes\n",
    "        \n",
    "    return vox_zoom, lbl_zoom\n",
    "\n",
    "def upscale_segmentation(lbl, shape_desired):\n",
    "    # Returns scaled up label for a given input label and desired shape. Required for Mean IOU calculation\n",
    "    \n",
    "    scale_factor = shape_desired[0]/lbl.shape[0]\n",
    "    lbl_upscale = scipy.ndimage.interpolation.zoom(lbl, scale_factor, order = 0)\n",
    "    # Order 0 EVEN more important here\n",
    "    lbl_upscale = lbl_upscale[:, :, :shape_desired[-1]]\n",
    "    if lbl_upscale.shape[-1] < shape_desired[-1]:\n",
    "        pad_zero = OFF_LABEL_FILL*np.zeros((shape_desired[0], shape_desired[1], shape_desired[2] - lbl_upscale.shape[-1]))\n",
    "        lbl_upscale = np.concatenate((lbl_upscale, pad_zero), axis = -1)\n",
    "    return lbl_upscale\n",
    "\n",
    "def get_label_accuracy(pred, lbl_original):\n",
    "    # Get pixel-wise labelling accuracy (DEMO metric)\n",
    "    \n",
    "    # Swap axes back\n",
    "    pred = swap_axes(pred)\n",
    "    pred_upscale = upscale_segmentation(pred, np.shape(lbl_original))\n",
    "    return 100*np.sum(np.equal(pred_upscale, lbl_original))/np.prod(lbl_original.shape)\n",
    "\n",
    "def get_mean_iou(pred, lbl_original, num_classes = OUTPUT_CLASSES, ret_full = False, reswap = False):\n",
    "    # Get mean IOU between input predictions and target labels. Note, method implicitly resizes as needed\n",
    "    # Ret_full - returns the full iou across all classes\n",
    "    # Reswap - if lbl_original is in tensorflow format, swap it back into the format expected by plotting tools (+ format of raw data)\n",
    "    \n",
    "    # Swap axes back \n",
    "    pred = swap_axes(pred)\n",
    "    if reswap:\n",
    "        lbl_original = swap_axes(lbl_original)\n",
    "    pred_upscale = upscale_segmentation(pred, np.shape(lbl_original))\n",
    "    iou = [1]*num_classes\n",
    "    for i in range(num_classes): \n",
    "        test_shape = np.zeros(np.shape(lbl_original))\n",
    "        test_shape[pred_upscale == i] = 1\n",
    "        test_shape[lbl_original == i] = 1\n",
    "        full_sum = int(np.sum(test_shape))\n",
    "        test_shape = -1*np.ones(np.shape(lbl_original))\n",
    "        test_shape[lbl_original == i] = pred_upscale[lbl_original == i]\n",
    "        t_p = int(np.sum(test_shape == i))\n",
    "        if not full_sum == 0:\n",
    "            iou[i] = t_p/full_sum\n",
    "    if ret_full:\n",
    "        return iou\n",
    "    else: \n",
    "        return np.mean(iou)\n",
    "    \n",
    "def swap_axes(pred):\n",
    "    # Swap those axes\n",
    "    pred = np.swapaxes(pred, -1, 0)\n",
    "    pred = np.squeeze(pred)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = './dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ATr_0A8a2c9s",
    "outputId": "383d53d7-f5ca-4c98-e727-55452504b245"
   },
   "outputs": [],
   "source": [
    "data = sio.loadmat(os.path.join(loc,'dataset.mat')) \n",
    "keys = sorted(data.keys())\n",
    "data = data[keys[3]] #shape: batch x 2 x h x w x d x ch (2 because 1 for data, another for the correspondint ground truth)\n",
    "\n",
    "print('data shape: ', data.shape)\n",
    "\n",
    "train = data[0:2004] \n",
    "test = data[2004:] \n",
    "\n",
    "\n",
    "print('train shape: ', train.shape)\n",
    "print('test shape: ', test.shape)\n",
    "\n",
    "del data\n",
    "\n",
    "'''\n",
    "data shape:  (2338, 2, 64, 64, 64, 1)\n",
    "train shape:  (2004, 2, 64, 64, 64, 1)\n",
    "test shape:  (334, 2, 64, 64, 64, 1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ACjjXyz2kNO"
   },
   "outputs": [],
   "source": [
    "# Extract train raw and label\n",
    "train_raw = train[:,0] #dtype: float32\n",
    "train_label = train[:,1] #dtype: int16\n",
    "train_label = train_label.astype('int8') #dtype: int8\n",
    "\n",
    "# Extract test raw and label\n",
    "test_raw = test[:,0] #dtype: float32\n",
    "test_label = test[:,1] #dtype: int16\n",
    "test_label = test_label.astype('int8') #dtype: int8\n",
    "\n",
    "del train, test\n",
    "\n",
    "print('Train shape: ', train_raw.shape)\n",
    "print('Test shape: ', test_raw.shape)\n",
    "\n",
    "'''\n",
    "Train shape:  (2004, 64, 64, 64, 1)\n",
    "Test shape:  (334, 64, 64, 64, 1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data_set, min_int, max_int):  \n",
    "    data_set = data_set.astype('float32')\n",
    "    data_set_norm = (data_set - min_int)/(max_int - min_int)\n",
    "    \n",
    "    return data_set_norm\n",
    "\n",
    "\n",
    "min_int, max_int = -1000, 3095 # min_int and max_int are the min and max value in the dataset\n",
    "\n",
    "train_raw = normalize(train_raw, min_int, max_int)\n",
    "test_raw = normalize(test_raw, min_int, max_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMAkc14WrU5k"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ueQDNS-rU5l"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001 # Model learning rate\n",
    "NUM_STEPS = 100000 # Number of train steps per model train\n",
    "BATCH_SIZE = 3 \n",
    "SAVE_PATH = './tf' #\"./tf/\" \n",
    "LOGS_PATH = './tf_logs' #\"./tf_logs/\"\n",
    "LOAD_MODEL = True\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)\n",
    "if not os.path.exists(LOGS_PATH):\n",
    "    os.makedirs(LOGS_PATH)\n",
    "MODEL_NAME = 'model' # Model name to LOAD FROM (LOOKS IN SAVE_PATH DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOOSUBeGrU5l"
   },
   "source": [
    "### Attention UNET Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLeUTUSUrU5l"
   },
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1.keras import backend as K\n",
    "from tensorflow.keras.layers import UpSampling3D\n",
    "from tensorflow.keras import models, layers, regularizers\n",
    "\n",
    "class UNetwork():\n",
    "    \n",
    "    def conv_batch_relu(self, tensor, filters, kernel = [3,3,3], stride = [1,1,1], is_training = True):\n",
    "        padding = 'valid'\n",
    "        if self.should_pad: padding = 'same'\n",
    "    \n",
    "        conv = tf.layers.conv3d(tensor, filters, kernel_size = kernel, strides = stride, padding = padding,\n",
    "                                kernel_initializer = self.base_init, kernel_regularizer = self.reg_init)\n",
    "        conv = tf.layers.batch_normalization(conv, training = is_training)\n",
    "        conv = tf.nn.relu(conv) \n",
    "        return conv\n",
    "\n",
    "    def upconvolve(self, tensor, filters, kernel = 2, stride = 2, scale = 4, activation = None):\n",
    "        padding = 'valid'\n",
    "        if self.should_pad: padding = 'same'\n",
    "\n",
    "        conv = tf.layers.conv3d_transpose(tensor, filters, kernel_size = kernel, strides = stride, padding = padding, use_bias=False, \n",
    "                                          kernel_initializer = self.base_init,  kernel_regularizer = self.reg_init)\n",
    "        return conv\n",
    "    \n",
    "    # ************************************** START: Attention mechanism ************************************** #\n",
    "    def repeat_elem(self, tensor, rep):\n",
    "\n",
    "         return layers.Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=4),\n",
    "                              arguments={'repnum': rep})(tensor) # pay attention to axis\n",
    "        \n",
    "    def gating_signal(self, input, out_size, batch_norm=False):\n",
    "\n",
    "        x = tf.layers.conv3d(inputs=input,filters=out_size, kernel_size=(1, 1, 1), strides=(1, 1, 1), padding='same')\n",
    "        if batch_norm:\n",
    "            x = tf.layers.batch_normalization(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "\n",
    "    def attention_block(self, x, gating, inter_shape):\n",
    "        shape_x = K.int_shape(x)\n",
    "        shape_g = K.int_shape(gating)\n",
    "\n",
    "        # Getting the x signal to the same shape as the gating signal\n",
    "        theta_x = tf.layers.conv3d(x, inter_shape, (2, 2, 2), strides=(2, 2, 2), padding='same')  # 16\n",
    "        shape_theta_x = K.int_shape(theta_x)\n",
    "\n",
    "        # Getting the gating signal to the same number of filters as the inter_shape\n",
    "        phi_g = tf.layers.conv3d(gating, inter_shape, (1, 1, 1), padding='same')\n",
    "\n",
    "        upsample_g = tf.layers.conv3d_transpose(phi_g, inter_shape, (3, 3, 3),\n",
    "                                     strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2], shape_theta_x[3] // shape_g[3]),\n",
    "                                     padding='same')  # 16\n",
    "\n",
    "        concat_xg = tf.keras.layers.add([upsample_g, theta_x]) \n",
    "        act_xg = tf.nn.relu(concat_xg)\n",
    "        psi = tf.layers.conv3d(act_xg, 1, (1, 1, 1), padding='same')\n",
    "        sigmoid_xg = tf.nn.sigmoid(psi)\n",
    "        shape_sigmoid = K.int_shape(sigmoid_xg)\n",
    "        upsample_psi = UpSampling3D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2], shape_x[3] // shape_sigmoid[3]))(sigmoid_xg)  # 32\n",
    "                                    \n",
    "        upsample_psi = self.repeat_elem(upsample_psi, shape_x[-1])  \n",
    "\n",
    "        y = tf.keras.layers.multiply([upsample_psi, x])\n",
    "\n",
    "        result = tf.layers.conv3d(y, shape_x[3], (1, 1, 1), padding='same')\n",
    "        result_bn = tf.layers.batch_normalization(result)\n",
    "        return result_bn\n",
    "    \n",
    "    # ************************************** END: Attention mechanism ************************************** #\n",
    "\n",
    "\n",
    "    def centre_crop_and_concat(self, prev_conv, up_conv):\n",
    "        # If concatenating two different sized Tensors, centre crop the first Tensor to the right size and concat\n",
    "        # Needed if you don't have padding\n",
    "        p_c_s = prev_conv.get_shape()\n",
    "        u_c_s = up_conv.get_shape()\n",
    "        offsets =  np.array([0, (p_c_s[1] - u_c_s[1]) // 2, (p_c_s[2] - u_c_s[2]) // 2, \n",
    "                             (p_c_s[3] - u_c_s[3]) // 2, 0], dtype = np.int32)\n",
    "        size = np.array([-1, u_c_s[1], u_c_s[2], u_c_s[3], p_c_s[4]], np.int32)\n",
    "        prev_conv_crop = tf.slice(prev_conv, offsets, size)\n",
    "        up_concat = tf.concat((prev_conv_crop, up_conv), 4)\n",
    "        return up_concat\n",
    "        \n",
    "    def __init__(self, base_filt = 8, in_depth = INPUT_DEPTH, out_depth = OUTPUT_DEPTH,\n",
    "                 in_size = INPUT_SIZE, out_size = OUTPUT_SIZE, num_classes = OUTPUT_CLASSES,\n",
    "                 learning_rate = LEARNING_RATE, print_shapes = True, drop = 0.2, should_pad = False):\n",
    "       \n",
    "        \n",
    "        self.base_init = tf.truncated_normal_initializer(stddev=0.1) # Initialise weights\n",
    "        self.reg_init = tf.keras.regularizers.l2(0.1) # Initialise regularisation (was useful)\n",
    "        \n",
    "        self.should_pad = should_pad # To pad or not to pad, that is the question\n",
    "        self.drop = drop # Set dropout rate\n",
    "        \n",
    "        with tf.variable_scope('3DuNet'):\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "            self.do_print = print_shapes\n",
    "            self.model_input = tf.placeholder(tf.float32, shape = (None, in_depth, in_size, in_size, 1))  \n",
    "            # Define placeholders for feed_dict\n",
    "            self.model_labels = tf.placeholder(tf.int32, shape = (None, in_depth, in_size, in_size, 1))\n",
    "            labels_one_hot = tf.squeeze(tf.one_hot(self.model_labels, num_classes, axis = -1), axis = -2)\n",
    "            \n",
    "            if self.do_print: \n",
    "                print('Input features shape', self.model_input.get_shape())\n",
    "                print('Labels shape', labels_one_hot.get_shape())\n",
    "                \n",
    "            # Level zero\n",
    "            conv_0_1 = self.conv_batch_relu(self.model_input, base_filt, is_training = self.training)\n",
    "            conv_0_2 = self.conv_batch_relu(conv_0_1, base_filt*2, is_training = self.training)\n",
    "            # Level one\n",
    "            max_1_1 = tf.layers.max_pooling3d(conv_0_2, [2,2,2], [2,2,2]) \n",
    "            conv_1_1 = self.conv_batch_relu(max_1_1, base_filt*2, is_training = self.training)\n",
    "            conv_1_2 = self.conv_batch_relu(conv_1_1, base_filt*4, is_training = self.training)\n",
    "            conv_1_2 = tf.layers.dropout(conv_1_2, rate = self.drop, training = self.training)\n",
    "            # Level two\n",
    "            max_2_1 = tf.layers.max_pooling3d(conv_1_2, [2,2,2], [2,2,2]) \n",
    "            conv_2_1 = self.conv_batch_relu(max_2_1, base_filt*4, is_training = self.training)\n",
    "            conv_2_2 = self.conv_batch_relu(conv_2_1, base_filt*8, is_training = self.training)\n",
    "            conv_2_2 = tf.layers.dropout(conv_2_2, rate = self.drop, training = self.training) \n",
    "            # Level three\n",
    "            max_3_1 = tf.layers.max_pooling3d(conv_2_2, [2,2,2], [2,2,2]) \n",
    "            conv_3_1 = self.conv_batch_relu(max_3_1, base_filt*8, is_training = self.training)\n",
    "            conv_3_2 = self.conv_batch_relu(conv_3_1, base_filt*16, is_training = self.training)\n",
    "            conv_3_2 = tf.layers.dropout(conv_3_2, rate = self.drop, training = self.training)\n",
    "            \n",
    "            # Level two\n",
    "            gating_1 = self.gating_signal(conv_3_2, base_filt*8, batch_norm=True)          \n",
    "            att_1 = self.attention_block(conv_2_2, gating_1, base_filt*8)           \n",
    "            up_conv_3_2 = self.upconvolve(conv_3_2, base_filt*16, kernel = 2, stride = [2,2,2]) \n",
    "            concat_2_1 = self.centre_crop_and_concat(att_1, up_conv_3_2)          \n",
    "            conv_2_3 = self.conv_batch_relu(concat_2_1, base_filt*8, is_training = self.training)\n",
    "            conv_2_4 = self.conv_batch_relu(conv_2_3, base_filt*8, is_training = self.training)\n",
    "            conv_2_4 = tf.layers.dropout(conv_2_4, rate = self.drop, training = self.training)\n",
    "            \n",
    "            # Level one\n",
    "            gating_2 = self.gating_signal(conv_2_4, base_filt*4, batch_norm=True) \n",
    "            att_2 = self.attention_block(conv_1_2, gating_2, base_filt*4)        \n",
    "            up_conv_2_1 = self.upconvolve(conv_2_4, base_filt*8, kernel = 2, stride = [2,2,2]) \n",
    "            concat_1_1 = self.centre_crop_and_concat(att_2, up_conv_2_1)\n",
    "            conv_1_3 = self.conv_batch_relu(concat_1_1, base_filt*4, is_training = self.training)\n",
    "            conv_1_4 = self.conv_batch_relu(conv_1_3, base_filt*4, is_training = self.training)\n",
    "            conv_1_4 = tf.layers.dropout(conv_1_4, rate = self.drop, training = self.training)\n",
    "            \n",
    "            # Level zero\n",
    "            gating_3 = self.gating_signal(conv_1_4, base_filt*2, batch_norm=True)\n",
    "            att_3 = self.attention_block(conv_0_2, gating_3, base_filt*2)           \n",
    "            up_conv_1_0 = self.upconvolve(conv_1_4, base_filt*4, kernel = 2, stride = [2,2,2])  \n",
    "            concat_0_1 = self.centre_crop_and_concat(att_3, up_conv_1_0)\n",
    "            conv_0_3 = self.conv_batch_relu(concat_0_1, base_filt*2, is_training = self.training)\n",
    "            conv_0_4 = self.conv_batch_relu(conv_0_3, base_filt*2, is_training = self.training)\n",
    "            conv_0_4 = tf.layers.dropout(conv_0_4, rate = self.drop, training = self.training)\n",
    "            conv_out = tf.layers.conv3d(conv_0_4, OUTPUT_CLASSES, [1,1,1], [1,1,1], padding = 'same')\n",
    "            self.predictions = tf.expand_dims(tf.argmax(conv_out, axis = -1), -1)\n",
    "            \n",
    "            if self.do_print: \n",
    "                print('Model Convolution output shape', conv_out.get_shape())\n",
    "                print('Model Argmax output shape', self.predictions.get_shape())\n",
    "            \n",
    "            do_weight = True\n",
    "            loss_weights = [1, 150, 100, 1.0]\n",
    "            # Weighted cross entropy: approach adapts following code: https://stackoverflow.com/questions/44560549/unbalanced-data-and-weighted-cross-entropy\n",
    "            ce_loss = tf.nn.softmax_cross_entropy_with_logits(logits=conv_out, labels=labels_one_hot)\n",
    "            if do_weight:\n",
    "                weighted_loss = tf.reshape(tf.constant(loss_weights), [1, 1, 1, 1, num_classes]) # Format to the right size\n",
    "                weighted_one_hot = tf.reduce_sum(weighted_loss*labels_one_hot, axis = -1)\n",
    "                ce_loss = ce_loss * weighted_one_hot\n",
    "            \n",
    "            self.loss = tf.reduce_mean(ce_loss) # Get loss\n",
    "            \n",
    "            self.trainer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            \n",
    "            self.extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # Ensure correct ordering for batch-norm to work\n",
    "            with tf.control_dependencies(self.extra_update_ops):\n",
    "                self.train_op = self.trainer.minimize(self.loss)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewEHBD-Hvyjp"
   },
   "source": [
    "### On-the-fly Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aug import rotation, flip, rot90, rot180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNMgaCg3v6Io"
   },
   "outputs": [],
   "source": [
    "def get_data_raw_sample_aug(raw, label, size):\n",
    "    # Convert arrays into list\n",
    "    rList = list(raw)\n",
    "    lbList = list(label)\n",
    "\n",
    "    idx = random.sample(range(len(rList)), k=size) #randomly peak some values \n",
    "\n",
    "    # Get raws and labels using idx\n",
    "    selected_raw = [rList[i] for i in idx] \n",
    "    selected_label = [lbList[i] for i in idx]\n",
    "    selected_raw = np.array(selected_raw)\n",
    "    selected_label = np.array(selected_label)\n",
    "\n",
    "    # Normalize data (*****Uncomment if you want to normalize*****)\n",
    "    # min_int = -1000\n",
    "    # max_int = 3095\n",
    "    # selected_raw = (selected_raw - min_int)/(max_int - min_int)\n",
    "    # selected_label = selected_label/255 #label is either 0 or 255\n",
    "\n",
    "    # selected_raw = selected_raw.astype('float32')\n",
    "    # selected_label = selected_label.astype('int8')\n",
    "  \n",
    "    # Do augmentation\n",
    "#    aug_val -> 0,1,2,3,4\n",
    "    aug_val = np.random.choice([0, 1],1) # np.random.choice([0, 1],1)\n",
    "    # aug_val -> 0    means original data\n",
    "    # aug_val -> 1    means rotation within a specific range\n",
    "    # aug_val -> 2    means rotation by 90 about a specific axis\n",
    "    # aug_val -> 3    means rotation by 180 about a specific axis\n",
    "    # aug_val -> 4    means flip by numpy\n",
    "\n",
    "    if (aug_val==0):\n",
    "      #print('No augmentation')\n",
    "      return list(selected_raw), list(selected_label)\n",
    "\n",
    "    elif (aug_val==1):\n",
    "      # Create random angles between -10 to +10      \n",
    "      for i in range(size):        \n",
    "        angle = np.random.randint(low=-10, high=10, size=(1,3))\n",
    "        #print('Rotated by {}, {}, {}'.format(angle[0][0],angle[0][1],angle[0][2]))\n",
    "        selected_raw[i], selected_label[i] = rotation(selected_raw[i], selected_label[i], \n",
    "                                                      (angle[0][0],angle[0][1],angle[0][2]))\n",
    "      return list(selected_raw), list(selected_label)\n",
    "\n",
    "    elif (aug_val==2):\n",
    "      for i in range(size):        \n",
    "        axis = np.random.choice([0,1,2], 1) # randomly peak an axis\n",
    "        selected_raw[i], selected_label[i] = rot90(selected_raw[i], selected_label[i], axis)\n",
    "      return list(selected_raw), list(selected_label)\n",
    "\n",
    "    elif (aug_val==3):\n",
    "      for i in range(size):        \n",
    "        axis = np.random.choice([0,1,2], 1) # randomly peak an axis\n",
    "        selected_raw[i], selected_label[i] = rot180(selected_raw[i], selected_label[i], axis)\n",
    "      return list(selected_raw), list(selected_label)   \n",
    "      \n",
    "    elif (aug_val==4):\n",
    "      for i in range(size):\n",
    "        flip_val = random.choice([0,2])                         \n",
    "        #print('Flipped along axis {}'.format(flip_val))                         \n",
    "        selected_raw[i], selected_label[i] = flip(selected_raw[i], selected_label[i], flip_val)\n",
    "          # flip_val -> 0    means flip along first axis\n",
    "          # flip_val -> 2    means flip along third axis\n",
    "      return list(selected_raw), list(selected_label)\n",
    "  \n",
    "    # print('random index: ', idx)\n",
    "    # print('max', np.max(selected_raw), np.max(selected_label))\n",
    "    # print(selected_raw.shape, selected_label.shape) # I added\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "    # x_y_data = random.sample(list(data), size)\n",
    "    # return [x[0] for x in x_y_data], [y[1] for y in x_y_data]\n",
    "\n",
    "# x, y = get_data_raw_sample_aug(train_raw, train_label, BATCH_SIZE) # Draw samples from batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWl5cjOWrU5l"
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZe-11forU5m",
    "outputId": "fe4df536-bb78-4127-dfd5-df5b9646a247"
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fGsq9NfirU5m",
    "outputId": "461a5d91-4497-4338-d73b-036cff68dec7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "unet = UNetwork(drop = 0.15, base_filt = 32, should_pad = True) # MODEL DEFINITION\n",
    "init = tf.global_variables_initializer()\n",
    "# saver = tf.train.Saver(tf.global_variables()) #this is original, I have commneted out\n",
    "saver = tf.train.Saver(tf.global_variables(), max_to_keep=None)\n",
    "config = tf.ConfigProto()\n",
    "st_loss = []\n",
    "with tf.Session(config=config) as sess:\n",
    "    writer = tf.summary.FileWriter(LOGS_PATH, graph=tf.get_default_graph())\n",
    "    if LOAD_MODEL:\n",
    "        print('Trying to load saved model...')\n",
    "        try:\n",
    "            print('Loading from: ', SAVE_PATH +'/'+ MODEL_NAME+ '.meta')\n",
    "            restorer = tf.train.import_meta_graph(SAVE_PATH +'/'+ MODEL_NAME+ '.meta')\n",
    "            restorer.restore(sess, tf.train.latest_checkpoint(SAVE_PATH))\n",
    "            print(\"Model sucessfully restored\")\n",
    "        except IOError:\n",
    "            sess.run(init)\n",
    "            print(\"No previous model found, running default init\") \n",
    "    t_loss = []\n",
    "    for i in range(NUM_STEPS):\n",
    "        print('Current iter: ', i, end='\\r')\n",
    "#         x, y, orig_y = get_dataset_sample(train, BATCH_SIZE, no_perturb = True) (USED IF DATA-AUG AT RUNTIME)\n",
    "        # x, y = get_data_raw_sample(train_run, BATCH_SIZE) # Draw samples from batch\n",
    "        x, y = get_data_raw_sample_aug(train_raw, train_label, BATCH_SIZE) #runtime augmentation function called\n",
    "        train_dict = {\n",
    "            unet.training: True,\n",
    "            unet.model_input: x,\n",
    "            unet.model_labels: y\n",
    "        }\n",
    "        _, loss = sess.run([unet.train_op, unet.loss], feed_dict = train_dict) # Get loss\n",
    "        t_loss.append(loss) # Loss store\n",
    "        # Store loss\n",
    "        if i % 500 == 0 and i > 0:\n",
    "            st_loss.append([i, loss])\n",
    "        if i>30000 and i % 500 == 0 and i > 0:\n",
    "            print('Saving model at iter: ', i) # Save periodically\n",
    "            saver.save(sess, SAVE_PATH + MODEL_NAME, global_step = i)\n",
    "        if i>30000 and i % 500 == 0 and i > 0:\n",
    "            print('Iteration', i, 'Loss: ', np.mean(t_loss)) # Get periodic progress reports\n",
    "            t_loss = []\n",
    "            iou_size = 5\n",
    "#             x, y, orig_y = get_dataset_sample(train, iou_size) (USED IF DATA-AUG AT RUNTIME)\n",
    "            # x, y = get_data_raw_sample(train_run, BATCH_SIZE)\n",
    "            x, y = get_data_raw_sample_aug(train_raw, train_label, BATCH_SIZE)\n",
    "            train_dict = {\n",
    "                unet.training: False,\n",
    "                unet.model_input: x,\n",
    "                unet.model_labels: y\n",
    "            }\n",
    "            preds = np.squeeze(sess.run([unet.predictions], feed_dict = train_dict))\n",
    "            iou = get_pred_iou(preds, y, ret_full = True, reswap = True)\n",
    "            print('Train IOU (on SCALED anns): ', iou, 'Mean: ', np.mean(iou[:OUTPUT_CLASSES-1]))\n",
    "            \n",
    "            # Get test mean IOU over batch\n",
    "            # x, y, orig_y = get_dataset_sample(test, iou_size, no_perturb = True)            \n",
    "            # train_dict = {\n",
    "            #     unet.training: False,\n",
    "            #     unet.model_input: x,\n",
    "            #     unet.model_labels: y\n",
    "            # }\n",
    "            # preds = np.squeeze(sess.run([unet.predictions], feed_dict = train_dict))\n",
    "            # iou = get_pred_iou(preds, orig_y, ret_full = True)\n",
    "            # print('Test IOU (on ORIGINAL anns): ', iou, 'Mean: ', np.mean(iou[:OUTPUT_CLASSES-1]))\n",
    "            print('######################')\n",
    "  \n",
    "    saver.save(sess,SAVE_PATH + MODEL_NAME, global_step = NUM_STEPS) # Final save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store loss\n",
    "sio.savemat('loss.mat', {'ls':st_loss}, do_compression=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OPc7QDQOrU5m",
    "outputId": "6e2ef32a-fcf2-462b-c201-56fe7af22cf1"
   },
   "outputs": [],
   "source": [
    "# For testing\n",
    "MODEL_PATH = './CBCT_Dental_NCI'\n",
    "MODEL_NAME = 'tfmodel-99500'\n",
    "\n",
    "# Activate tensorflow v1\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Load model\n",
    "TEST_MODEL_NAME = MODEL_NAME\n",
    "tf.reset_default_graph()\n",
    "unet = UNetwork(drop = 0.15, base_filt = 32, should_pad = True) # MODEL DEFINITION\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "test_predictions = []\n",
    "restorer = tf.train.Saver(tf.global_variables())\n",
    "with tf.Session(config=config) as sess:\n",
    "    # tf.initialize_all_variables().run()\n",
    "    print('Loading saved model ...')\n",
    "#   try\n",
    "    graph = tf.get_default_graph()\n",
    "    restorer.restore(sess, SAVE_PATH +'/'+ TEST_MODEL_NAME)\n",
    "#     restorer = tf.train.import_meta_graph(MODEL_PATH +'/'+ TEST_MODEL_NAME + '.meta')    \n",
    "#     restorer.restore(sess, tf.train.latest_checkpoint(MODEL_PATH))\n",
    "    print(\"Model sucessfully restored\")\n",
    "    pred_out = []\n",
    "    y_orig = []\n",
    "    x_orig = []\n",
    "    x_in = []\n",
    "    y_in = []\n",
    "    i = 0\n",
    "    iou_out = []\n",
    "\n",
    "    while i < len(test_raw):\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for j in range(i, min(len(test_raw), i + BATCH_SIZE)):\n",
    "            y_orig.append(np.copy(test_label[j]))\n",
    "            x_orig.append(np.copy(test_raw[j]))\n",
    "            x_cur, y_cur = test_raw[j], test_label[j]\n",
    "\n",
    "            x_batch.append(x_cur)\n",
    "            y_batch.append(y_cur)\n",
    "        if len(x_batch) == 0: break\n",
    "        print('Processing ', i)\n",
    "        x_in = x_in + x_batch\n",
    "        y_in = y_in + y_batch\n",
    "        test_dict = {\n",
    "            unet.training: True, # Whether to perform batch-norm at inference (Paper says this would be useful)\n",
    "            unet.model_input: x_batch,\n",
    "            unet.model_labels: y_batch\n",
    "        }\n",
    "        test_predictions = np.squeeze(sess.run([unet.predictions], feed_dict = test_dict))\n",
    "        if len(x_batch) == 1:\n",
    "            pred_out.append(test_predictions)\n",
    "        else:\n",
    "            pred_out.extend([np.squeeze(test_predictions[z, :, :, :]) for z in list(range(len(x_batch)))])\n",
    "        i += BATCH_SIZE\n",
    "\n",
    "    for i in range(len(y_orig)):\n",
    "        iou = get_mean_iou(pred_out[i], np.squeeze(y_orig[i]), ret_full = True)\n",
    "        print('Test IOU: ', iou, 'Mean: ', np.mean(iou[:OUTPUT_CLASSES-1]))\n",
    "        iou_out.append(np.mean(iou[:OUTPUT_CLASSES-1]))\n",
    "\n",
    "    print('Mean test IOU', np.mean(iou_out), 'Var IOU', np.var(iou_out))\n",
    "\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print('Something went wrong!', e)\n",
    "\n",
    "pred_out = np.array(pred_out).astype('int8')\n",
    "sio.savemat(os.path.join(MODEL_PATH, 'pred_' + MODEL_NAME + '.mat'), {'p':pred_out}, do_compression=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "CBCT_3D_unet2_2_nci_patch_64_aug_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
